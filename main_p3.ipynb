{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Potential Talents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal(s):**\n",
    "\n",
    "Determine best matching candidates based on how fit these candidates are for a given role:\n",
    "- Predict how fit the candidate is based on their available information (variable fit).\n",
    "- Rank candidates based on a fitness score.\n",
    "- Re-rank candidates when a candidate is starred.\n",
    "\n",
    "Attributes:\n",
    "\n",
    "- id : unique identifier for candidate (numeric)\n",
    "- job_title : job title for candidate (text)\n",
    "- location : geographical location for candidate (text)\n",
    "- connections: number of connections candidate has, 500+ means over 500 (text)\n",
    "\n",
    "Output (desired target):\n",
    "\n",
    "- fit : how fit the candidate is for the role? (numeric, probability between 0-1)\n",
    "\n",
    "**Results:**\n",
    "\n",
    "\n",
    "<!-- # TODO -->\n",
    "<!-- - We are also interested in finding customers who are more likely to buy the investment product. Determine the segment(s) of customers our client should prioritize. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from state_information import USA_STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1   2  Native English Teacher at EPIK (English Progra...   \n",
       "2   3              Aspiring Human Resources Professional   \n",
       "3   4             People Development Coordinator at Ryan   \n",
       "4   5    Advisory Board Member at Celal Bayar University   \n",
       "\n",
       "                              location connection   Y  \n",
       "0                       Houston, Texas         85 NaN  \n",
       "1                               Kanada      500+  NaN  \n",
       "2  Raleigh-Durham, North Carolina Area         44 NaN  \n",
       "3                        Denton, Texas      500+  NaN  \n",
       "4                       İzmir, Türkiye      500+  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 104 entries, 0 to 103\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          104 non-null    int64  \n",
      " 1   job_title   104 non-null    object \n",
      " 2   location    104 non-null    object \n",
      " 3   connection  104 non-null    object \n",
      " 4   Y           0 non-null      float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 4.2+ KB\n"
     ]
    }
   ],
   "source": [
    "file_name = 'Seeking_human_resources.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "df = df.rename({'fit': 'Y'}, axis=1)\n",
    "\n",
    "feature_names = df.columns\n",
    "\n",
    "display(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our task is to complete de Y (fit) column, we should start by doing some data wrangling on columns job_title, location and connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values between 1-500+\n"
     ]
    }
   ],
   "source": [
    "connection_values = ['501' if x == '500+ ' else x for x in list(df['connection'].values)]\n",
    "connection_values = list(map(int, connection_values))\n",
    "print(f\"Values between {np.min(connection_values)}-500+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we will re-organize data into three categories:\n",
    "- `100-`: less than 100 connections\n",
    "- `100-500`: between 100 and 500 connections\n",
    "- `500+`: more than 500 connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100-       49\n",
       "500+       44\n",
       "100-500    11\n",
       "Name: connection, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idd in df['id']:\n",
    "    connection = df.loc[df['id'] == idd, 'connection'].values[0]\n",
    "    if connection == '500+ ': df.loc[df['id'] == idd, 'connection'] = '500+'\n",
    "    else:\n",
    "        if (int(connection) <= 500) and (int(connection) >= 100): df.loc[df['id'] == idd, 'connection'] = '100-500'\n",
    "        else: df.loc[df['id'] == idd, 'connection'] = '100-'\n",
    "\n",
    "print('Value counts:')\n",
    "df['connection'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kanada                                 12\n",
       "Raleigh-Durham, North Carolina Area     8\n",
       "Houston, Texas Area                     8\n",
       "Greater New York City Area              7\n",
       "Houston, Texas                          7\n",
       "Denton, Texas                           6\n",
       "San Francisco Bay Area                  5\n",
       "Greater Philadelphia Area               5\n",
       "İzmir, Türkiye                          4\n",
       "Lake Forest, California                 4\n",
       "Atlanta, Georgia                        4\n",
       "Chicago, Illinois                       2\n",
       "Austin, Texas Area                      2\n",
       "Greater Atlanta Area                    2\n",
       "Amerika Birleşik Devletleri             2\n",
       "Long Beach, California                  1\n",
       "Milpitas, California                    1\n",
       "Greater Chicago Area                    1\n",
       "Torrance, California                    1\n",
       "Greater Los Angeles Area                1\n",
       "Bridgewater, Massachusetts              1\n",
       "Lafayette, Indiana                      1\n",
       "Kokomo, Indiana Area                    1\n",
       "Las Vegas, Nevada Area                  1\n",
       "Cape Girardeau, Missouri                1\n",
       "Gaithersburg, Maryland                  1\n",
       "Baltimore, Maryland                     1\n",
       "Dallas/Fort Worth Area                  1\n",
       "Highland, California                    1\n",
       "Los Angeles, California                 1\n",
       "Chattanooga, Tennessee Area             1\n",
       "Myrtle Beach, South Carolina Area       1\n",
       "Baton Rouge, Louisiana Area             1\n",
       "New York, New York                      1\n",
       "San Jose, California                    1\n",
       "Greater Boston Area                     1\n",
       "Monroe, Louisiana Area                  1\n",
       "Virginia Beach, Virginia                1\n",
       "Greater Grand Rapids, Michigan Area     1\n",
       "Jackson, Mississippi Area               1\n",
       "Katy, Texas                             1\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to add information that will be useful to detected certain paterns in text. In this case we will add the following information:\n",
    "- Add alternative spellings\n",
    "- Add english translation\n",
    "- Add US regions, divisions and state acronyms\n",
    "\n",
    "We will create a new column called `location_key_words` where all keywords will be accumulated for each `location` row. And the `location` attribute will be left with a clean version of the location for presentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location_key_words'] = df['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_key_attribute(df, original_value, new_value, clean_value, original_att='location', key_att='location_key_words'):\n",
    "    df.loc[df[original_att] == original_value, key_att] = df.loc[df[original_att] == original_value, key_att] + ', ' + new_value\n",
    "    df.loc[df[original_att] == original_value, original_att] = clean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_key_attribute(df, original_value='Kanada', new_value='Canada', clean_value='Canada')\n",
    "add_to_key_attribute(df, original_value='İzmir, Türkiye', new_value='Izmir, Turkey', clean_value='Izmir, Turkey')\n",
    "add_to_key_attribute(df, original_value='Amerika Birleşik Devletleri', new_value='United States of America, USA, US', clean_value='USA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add state acronyms and region according to the US Census Bureau we can divide the US states into 4 Regions:\n",
    "<br><https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf>\n",
    "<br><https://www.scouting.org/resources/los/states/>\n",
    "\t \n",
    "| West | Midwest | South | Northeast |\n",
    "|-----------|---------|-------|------|\n",
    "| Arizona | Indiana | Delaware | Connecticut |\n",
    "| Colorado | Illinois | District of Columbia | Maine |\n",
    "| Idaho | Michigan | Florida | Massachusetts |\n",
    "| New Mexico | Ohio | Georgia | New Hampshire |\n",
    "| Montana | Wisconsin | Maryland | Rhode Island |\n",
    "| Utah | Iowa | North Carolina | Vermont |\n",
    "| Nevada | Kansas | South Carolina | New Jersey |\n",
    "| Wyoming | Minnesota | Virginia | New York |\n",
    "| Alaska | Missouri | West Virginia | Pennsylvania |\n",
    "| California | Nebraska | Alabama |  |\n",
    "| Hawaii | North Dakota | Kentucky |  |\n",
    "| Oregon | South Dakota | Mississippi |  |\n",
    "| Washington |  | Tennessee |  |\n",
    "|  |  | Arkansas |  |\n",
    "|  |  | Louisiana |  |\n",
    "|  |  | Oklahoma |  |\n",
    "|  |  | Texas |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>Y</th>\n",
       "      <th>location_key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>100-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Houston, Texas, Texas, TX, South, S, West Sout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Canada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kanada, Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>100-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area, N.C., NC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Denton, Texas, Texas, TX, South, S, West South...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>Izmir, Turkey</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>İzmir, Türkiye, Izmir, Turkey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1   2  Native English Teacher at EPIK (English Progra...   \n",
       "2   3              Aspiring Human Resources Professional   \n",
       "3   4             People Development Coordinator at Ryan   \n",
       "4   5    Advisory Board Member at Celal Bayar University   \n",
       "\n",
       "                              location connection   Y  \\\n",
       "0                       Houston, Texas       100- NaN   \n",
       "1                               Canada       500+ NaN   \n",
       "2  Raleigh-Durham, North Carolina Area       100- NaN   \n",
       "3                        Denton, Texas       500+ NaN   \n",
       "4                        Izmir, Turkey       500+ NaN   \n",
       "\n",
       "                                  location_key_words  \n",
       "0  Houston, Texas, Texas, TX, South, S, West Sout...  \n",
       "1                                     Kanada, Canada  \n",
       "2  Raleigh-Durham, North Carolina Area, N.C., NC,...  \n",
       "3  Denton, Texas, Texas, TX, South, S, West South...  \n",
       "4                      İzmir, Türkiye, Izmir, Turkey  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for loc in df['location'].value_counts().index:\n",
    "\n",
    "    split = loc.split(' ') # split location value by ' ' to find state key words\n",
    "\n",
    "    # Special cases include states with two words or cities.\n",
    "    # In the latter case, the split value was replaced by the state name where that city belongs to\n",
    "    if 'North Carolina' in loc: split = ['North Carolina']\n",
    "    elif 'New York' in loc: split = ['New York']\n",
    "    elif ('San Francisco' in loc) and ('California' not in loc): split = ['California']\n",
    "    elif ('Philadelphia' in loc) and ('Pennsylvania' not in loc): split = ['Pennsylvania']\n",
    "    elif ('Chicago' in loc) and ('Illinois' not in loc): split = ['Illinois']\n",
    "    elif ('Los Angeles' in loc) and ('California' not in loc): split = ['California']\n",
    "    elif ('Dallas/Fort Worth' in loc) and ('Texas' not in loc): split = ['Texas']\n",
    "    elif ('Boston' in loc) and ('Massachusetts' not in loc): split = ['Massachusetts']\n",
    "    elif ('Dallas/Fort Worth' in loc) and ('Texas' not in loc): split = ['Texas']\n",
    "\n",
    "    # If it a special case, the length should be = 1 and the the split value should correspond to 1 USA_STATES key\n",
    "    if (len(split) == 1) and (split[0] in USA_STATES.keys()):\n",
    "        nv = USA_STATES[split[0]]['Standard'] + ', ' + USA_STATES[split[0]]['Postal'] + ', ' + USA_STATES[split[0]]['Region']\n",
    "        add_to_key_attribute(df, original_value=loc, new_value=nv, clean_value=loc)\n",
    "\n",
    "    # In this case split has a length of one but doesn't have value that corresponds to 1 USA_STATES key\n",
    "    elif (len(split) == 1) and (split[0] not in USA_STATES.keys()): pass\n",
    "\n",
    "    # If it isn't a special case, split's length will be > than 1. We iterate through all keywords stored in split.\n",
    "    # If the keyword corresponds to 1 USA_STATES key, add_to_key_attribute\n",
    "    else:\n",
    "        for s in split:\n",
    "            if s in USA_STATES.keys():\n",
    "                nv = USA_STATES[s]['Standard'] + ', ' + USA_STATES[s]['Postal'] + ', ' + USA_STATES[s]['Region']\n",
    "                add_to_key_attribute(df, original_value=loc, new_value=nv, clean_value=loc)\n",
    "\n",
    "df.head()  # TODO shouold I eliminate duplicates in the location_key_words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add alternative words for some cities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in df['location'].value_counts().index:\n",
    "    if 'New York City' in loc:   add_to_key_attribute(df, original_value=loc, new_value='NYC', clean_value=loc)\n",
    "    if 'Philadelphia' in loc:   add_to_key_attribute(df, original_value=loc, new_value='Philly', clean_value=loc)\n",
    "    if 'Los Angeles' in loc:   add_to_key_attribute(df, original_value=loc, new_value='LA', clean_value=loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert all words into lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>Y</th>\n",
       "      <th>location_key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 c.t. bauer college of business graduate (...</td>\n",
       "      <td>houston, texas</td>\n",
       "      <td>100-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>houston, texas, texas, tx, south, s, west sout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>native english teacher at epik (english progra...</td>\n",
       "      <td>canada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kanada, canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aspiring human resources professional</td>\n",
       "      <td>raleigh-durham, north carolina area</td>\n",
       "      <td>100-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>raleigh-durham, north carolina area, n.c., nc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>people development coordinator at ryan</td>\n",
       "      <td>denton, texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>denton, texas, texas, tx, south, s, west south...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>advisory board member at celal bayar university</td>\n",
       "      <td>izmir, turkey</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i̇zmir, türkiye, izmir, turkey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 c.t. bauer college of business graduate (...   \n",
       "1   2  native english teacher at epik (english progra...   \n",
       "2   3              aspiring human resources professional   \n",
       "3   4             people development coordinator at ryan   \n",
       "4   5    advisory board member at celal bayar university   \n",
       "\n",
       "                              location connection   Y  \\\n",
       "0                       houston, texas       100- NaN   \n",
       "1                               canada       500+ NaN   \n",
       "2  raleigh-durham, north carolina area       100- NaN   \n",
       "3                        denton, texas       500+ NaN   \n",
       "4                        izmir, turkey       500+ NaN   \n",
       "\n",
       "                                  location_key_words  \n",
       "0  houston, texas, texas, tx, south, s, west sout...  \n",
       "1                                     kanada, canada  \n",
       "2  raleigh-durham, north carolina area, n.c., nc,...  \n",
       "3  denton, texas, texas, tx, south, s, west south...  \n",
       "4                     i̇zmir, türkiye, izmir, turkey  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['job_title', 'location', 'location_key_words']] = df[['job_title', 'location', 'location_key_words']].applymap(lambda x: x.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019 c.t. bauer college of business graduate (magna cum laude) and aspiring human resources professional                 7\n",
       "aspiring human resources professional                                                                                    7\n",
       "student at humber college and aspiring human resources generalist                                                        7\n",
       "people development coordinator at ryan                                                                                   6\n",
       "native english teacher at epik (english program in korea)                                                                5\n",
       "aspiring human resources specialist                                                                                      5\n",
       "hr senior specialist                                                                                                     5\n",
       "student at chapman university                                                                                            4\n",
       "svp, chro, marketing & communications, csr officer | engie | houston | the woodlands | energy | gphr | sphr              4\n",
       "human resources coordinator at intercontinental buckhead atlanta                                                         4\n",
       "seeking human resources hris and generalist positions                                                                    4\n",
       "advisory board member at celal bayar university                                                                          4\n",
       "aspiring human resources management student seeking an internship                                                        2\n",
       "seeking human resources opportunities                                                                                    2\n",
       "seeking human  resources opportunities. open to travel and relocation.                                                   1\n",
       "bachelor of science in biology from victoria university of wellington                                                    1\n",
       "human resources management major                                                                                         1\n",
       "director human resources  at ey                                                                                          1\n",
       "undergraduate research assistant at styczynski lab                                                                       1\n",
       "lead official at western illinois university                                                                             1\n",
       "seeking employment opportunities within customer service or patient care                                                 1\n",
       "admissions representative at community medical center long beach                                                         1\n",
       "human resources generalist at loparex                                                                                    1\n",
       "student at westfield state university                                                                                    1\n",
       "student at indiana university kokomo - business management - \\nretail manager at delphi hardware and paint               1\n",
       "student                                                                                                                  1\n",
       "seeking human resources position                                                                                         1\n",
       "aspiring human resources manager | graduating may 2020 | seeking an entry-level human resources position in st. louis    1\n",
       "rrp brand portfolio executive at jti (japan tobacco international)                                                       1\n",
       "business intelligence and analytics at travelers                                                                         1\n",
       "always set them up for success                                                                                           1\n",
       "information systems specialist and programmer with a love for data and organization.                                     1\n",
       "human resources generalist at schwan's                                                                                   1\n",
       "human resources professional for the world leader in gis software                                                        1\n",
       "aspiring human resources manager, seeking internship in human resources.                                                 1\n",
       "experienced retail manager and aspiring human resources professional                                                     1\n",
       "human resources, staffing and recruiting professional                                                                    1\n",
       "human resources specialist at luxottica                                                                                  1\n",
       "director of human resources north america, groupe beneteau                                                               1\n",
       "retired army national guard recruiter, office manager,  seeking a position in human resources.                           1\n",
       "human resources generalist at scottmadden, inc.                                                                          1\n",
       "business management major and aspiring human resources manager                                                           1\n",
       "human resources professional                                                                                             1\n",
       "hr manager at endemol shine north america                                                                                1\n",
       "nortia staffing is seeking human resources, payroll & administrative professionals!!  (408) 709-2621                     1\n",
       "aspiring human resources professional | passionate about helping to create an inclusive and engaging work environment    1\n",
       "human resources|\\nconflict management|\\npolicies & procedures|talent management|benefits & compensation                  1\n",
       "liberal arts major. aspiring human resources analyst.                                                                    1\n",
       "junior mes engineer| information systems                                                                                 1\n",
       "senior human resources business partner at heil environmental                                                            1\n",
       "aspiring human resources professional | an energetic and team-focused leader                                             1\n",
       "director of administration at excellence logging                                                                         1\n",
       "Name: job_title, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['job_title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add some abbreviations to this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_abbreviations_to_job_attribute(df, original_value, new_value):\n",
    "    df.loc[df['job_title'] == original_value, 'job_title'] = df.loc[df['job_title'] == original_value, 'job_title'] + ', ' + new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>Y</th>\n",
       "      <th>location_key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 c.t. bauer college of business graduate (...</td>\n",
       "      <td>houston, texas</td>\n",
       "      <td>100-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>houston, texas, texas, tx, south, s, west sout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>native english teacher at epik (english progra...</td>\n",
       "      <td>canada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kanada, canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>aspiring human resources professional, hr, hum...</td>\n",
       "      <td>raleigh-durham, north carolina area</td>\n",
       "      <td>100-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>raleigh-durham, north carolina area, n.c., nc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>people development coordinator at ryan</td>\n",
       "      <td>denton, texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>denton, texas, texas, tx, south, s, west south...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>advisory board member at celal bayar universit...</td>\n",
       "      <td>izmir, turkey</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i̇zmir, türkiye, izmir, turkey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 c.t. bauer college of business graduate (...   \n",
       "1   2  native english teacher at epik (english progra...   \n",
       "2   3  aspiring human resources professional, hr, hum...   \n",
       "3   4             people development coordinator at ryan   \n",
       "4   5  advisory board member at celal bayar universit...   \n",
       "\n",
       "                              location connection   Y  \\\n",
       "0                       houston, texas       100- NaN   \n",
       "1                               canada       500+ NaN   \n",
       "2  raleigh-durham, north carolina area       100- NaN   \n",
       "3                        denton, texas       500+ NaN   \n",
       "4                        izmir, turkey       500+ NaN   \n",
       "\n",
       "                                  location_key_words  \n",
       "0  houston, texas, texas, tx, south, s, west sout...  \n",
       "1                                     kanada, canada  \n",
       "2  raleigh-durham, north carolina area, n.c., nc,...  \n",
       "3  denton, texas, texas, tx, south, s, west south...  \n",
       "4                     i̇zmir, türkiye, izmir, turkey  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for job in df['job_title']:\n",
    "    if 'hr' in job: add_abbreviations_to_job_attribute(df, job, 'human resources')\n",
    "    if 'human resources' in job: add_abbreviations_to_job_attribute(df, job, 'hr')\n",
    "    if ('sr.' in job) or ('sr ' in job): add_abbreviations_to_job_attribute(df, job, 'senior')\n",
    "    if 'senior' in job: add_abbreviations_to_job_attribute(df, job, 'Sr')\n",
    "    if ('jr.' in job) or ('jr ' in job): add_abbreviations_to_job_attribute(df, job, 'junior')\n",
    "    if 'junior' in job: add_abbreviations_to_job_attribute(df, job, 'jr')\n",
    "    if 'entry-level' in job: add_abbreviations_to_job_attribute(df, job, 'entry level')\n",
    "    if 'business intelligence' in job: add_abbreviations_to_job_attribute(df, job, 'bi')\n",
    "    if 'bi' in job: add_abbreviations_to_job_attribute(df, job, 'business intelligence')\n",
    "    if 'information systems' in job: add_abbreviations_to_job_attribute(df, job, 'it')\n",
    "    if 'it' in job: add_abbreviations_to_job_attribute(df, job, 'information technology systems')\n",
    "    if 'engineer' in job: add_abbreviations_to_job_attribute(df, job, 'eng')\n",
    "    if 'bachelor of science' in job: add_abbreviations_to_job_attribute(df, job, 'bs')\n",
    "    if 'hris' in job: add_abbreviations_to_job_attribute(df, job, 'human resources information systems hr it technology')\n",
    "    if 'gis' in job: add_abbreviations_to_job_attribute(df, job, 'geographic information system it technology')\n",
    "    if 'rrp' in job: add_abbreviations_to_job_attribute(df, job, 'recommended retail price')\n",
    "    if 'mes' in job: add_abbreviations_to_job_attribute(df, job, 'manufacturing execution system')\n",
    "    if 'svp' in job: add_abbreviations_to_job_attribute(df, job, 'senior vice president sr')\n",
    "    if 'senior vice president' in job: add_abbreviations_to_job_attribute(df, job, 'svp sr')\n",
    "    if 'chief human resources officer' in job: add_abbreviations_to_job_attribute(df, job, 'chro hr')\n",
    "    if 'chro' in job: add_abbreviations_to_job_attribute(df, job, 'chief human resources officer hr')\n",
    "    if 'csr' in job: add_abbreviations_to_job_attribute(df, job, 'corporate social responsibility')\n",
    "    if 'corporate social responsibility' in job: add_abbreviations_to_job_attribute(df, job, 'csr')\n",
    "    if 'gphr' in job: add_abbreviations_to_job_attribute(df, job, 'global professional in human resources hr')\n",
    "    if 'global professional in human resources' in job: add_abbreviations_to_job_attribute(df, job, 'gphr hr')\n",
    "    if 'sphr' in job: add_abbreviations_to_job_attribute(df, job, 'senior professional in human resources hr sr')\n",
    "    if 'senior professional in human resources' in job: add_abbreviations_to_job_attribute(df, job, 'sphr hr sr')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we finished adding keywords, we continue with the data preprocessing. We have already converted words into lower case words. We will also apply lemmatization and remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # word_tokenize = split(' '); sent_tokenize = split('.')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    # print(df[0])\n",
    "    print('[INFO] Splitting sentence into array...')\n",
    "    df = df.apply(lambda x: word_tokenize(x.lower()))   # tokenize: split sentence into array of words\n",
    "    # print(df[0])\n",
    "    \n",
    "    print('[INFO] Removing special characters and punctuation...')\n",
    "    df = df.apply(lambda x: list(set(word_tokenize(re.sub('[^A-Za-z*$]', ' ', str(x)))))) # remove special characters and digits\n",
    "    # print(df[0])\n",
    "\n",
    "    print('[INFO] Lemmatizing words...')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df = df.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    # print(df[0])\n",
    "    \n",
    "    stopwords = stopwords.words('english')\n",
    "    # df = df.apply(lambda x: [word for word in x if word not in stopwords])\n",
    "    # print(df[0])\n",
    "\n",
    "    print('[INFO] Removing stopwords and generating embeddings vector...')\n",
    "    df = df.apply(lambda x: ' '.join(x))\n",
    "    # display(df[0])\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words = stopwords, lowercase = False, strip_accents = 'unicode')\n",
    "    encoding = vectorizer.fit_transform(df)\n",
    "    encoding_df = pd.DataFrame(encoding.todense(), columns = vectorizer.get_feature_names_out())\n",
    "    # display(encoding_df)\n",
    "\n",
    "    vectorizer = TfidfTransformer()\n",
    "    encoding = vectorizer.fit_transform(encoding_df)\n",
    "    encoding_df = pd.DataFrame(encoding.todense(), columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords)  # can remove words that appear too unfrequently and too frequently (min_df and max_df)\n",
    "    encoding = vectorizer.fit_transform(df)\n",
    "    encoding_df = pd.DataFrame(encoding.todense(), columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "    # If you need the term frequency (term count) vectors for different tasks, use Tfidftransformer.\n",
    "    # If you need to compute tf-idf scores on documents within your “training” dataset, use Tfidfvectorizer\n",
    "    # If you need to compute tf-idf scores on documents outside your “training” dataset, use either one, both will work.\n",
    "    # https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.Y5UtcdKZPb0\n",
    "\n",
    "    return encoding, encoding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting sentence into array...\n",
      "[INFO] Removing special characters and punctuation...\n",
      "[INFO] Lemmatizing words...\n",
      "[INFO] Removing stopwords and generating embeddings vector...\n",
      "[INFO] Splitting sentence into array...\n",
      "[INFO] Removing special characters and punctuation...\n",
      "[INFO] Lemmatizing words...\n",
      "[INFO] Removing stopwords and generating embeddings vector...\n"
     ]
    }
   ],
   "source": [
    "encoding, encoding_df = preprocess_text(df['location_key_words'])\n",
    "query_encoding, query_encoding_df = preprocess_text(pd.Series('HR Specialist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting sentence into array...\n",
      "[INFO] Removing special characters and punctuation...\n",
      "[INFO] Lemmatizing words...\n",
      "[INFO] Removing stopwords and generating embeddings vector...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/alexandrabinder/Projects/Apziva/P3_Potential_Talents/main_p3.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexandrabinder/Projects/Apziva/P3_Potential_Talents/main_p3.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mlocation_key_words\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexandrabinder/Projects/Apziva/P3_Potential_Talents/main_p3.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     encoding, encoding_df \u001b[39m=\u001b[39m preprocess_text(pd\u001b[39m.\u001b[39mSeries(entry))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexandrabinder/Projects/Apziva/P3_Potential_Talents/main_p3.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     cosine \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(query_encoding, encoding)\u001b[39m/\u001b[39m(norm(query_encoding)\u001b[39m*\u001b[39mnorm(encoding))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexandrabinder/Projects/Apziva/P3_Potential_Talents/main_p3.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCosine Similarity:\u001b[39m\u001b[39m\"\u001b[39m, cosine)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/apziva-p1/lib/python3.10/site-packages/scipy/sparse/_base.py:590\u001b[0m, in \u001b[0;36mspmatrix.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__mul__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 590\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[0;32m~/anaconda3/envs/apziva-p1/lib/python3.10/site-packages/scipy/sparse/_base.py:540\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m issparse(other):\n\u001b[1;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 540\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdimension mismatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    541\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_sparse_matrix(other)\n\u001b[1;32m    543\u001b[0m \u001b[39m# If it's a list or whatever, treat it like a matrix\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "for entry in df['location_key_words']:\n",
    "    encoding, encoding_df = preprocess_text(pd.Series(entry))\n",
    "    cosine = np.dot(query_encoding, encoding)/(norm(query_encoding)*norm(encoding))\n",
    "    print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 2 while Y.shape[1] == 112",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/alexandrabinder/Projects/Apziva/P3_Potential_Talents/main_p3.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexandrabinder/Projects/Apziva/P3_Potential_Talents/main_p3.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexandrabinder/Projects/Apziva/P3_Potential_Talents/main_p3.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cosine_similarity(query_encoding, encoding)\n",
      "File \u001b[0;32m~/anaconda3/envs/apziva-p1/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1377\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \n\u001b[1;32m   1344\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1379\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1380\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/anaconda3/envs/apziva-p1/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:180\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    175\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPrecomputed metric requires shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(n_queries, n_indexed). Got (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfor \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m indexed.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    178\u001b[0m         )\n\u001b[1;32m    179\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 180\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mX.shape[1] == \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m while Y.shape[1] == \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m X, Y\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 2 while Y.shape[1] == 112"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(query_encoding, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension 1:\n",
      "['aspiring' 'resources' 'human' 'professional' 'college' 'specialist'\n",
      " 'generalist' 'student' 'humber' 'seeking' 'business' 'magna' 'bauer'\n",
      " 'laude' 'graduate' 'cum' '2019' 'management' 'internship' 'positions'\n",
      " 'hris' 'manager' 'opportunities' 'major' 'buckhead' 'atlanta'\n",
      " 'intercontinental' 'senior' 'coordinator' 'hr' 'position' 'chapman'\n",
      " 'university' 'loparex' 'schwan' 'luxottica' 'leader' 'retail'\n",
      " 'experienced' 'staffing' 'recruiting' 'director' 'scottmadden' 'inc'\n",
      " 'team' 'energetic' 'focused' 'ey' 'liberal' 'arts' 'analyst' 'world'\n",
      " 'gis' 'software' 'may' 'louis' 'level' 'entry' 'st' '2020' 'graduating'\n",
      " 'open' 'relocation' 'travel' 'work' 'environment' 'engaging' 'create'\n",
      " 'inclusive' 'helping' 'passionate' 'heil' 'partner' 'environmental'\n",
      " 'america' 'north' 'beneteau' 'groupe' 'army' 'office' 'guard' 'retired'\n",
      " 'recruiter' 'national' 'development' 'people' 'ryan' '709' 'nortia'\n",
      " 'professionals' 'payroll' 'administrative' '408' '2621' 'conflict'\n",
      " 'policies' 'compensation' 'procedures' 'benefits' 'talent' 'westfield'\n",
      " 'state' 'delphi' 'hardware' 'kokomo' 'paint' 'indiana' 'programmer'\n",
      " 'organization' 'love' 'data' 'systems' 'information' 'analytics'\n",
      " 'travelers' 'intelligence' 'care' 'customer' 'employment' 'service'\n",
      " 'patient' 'within' 'shine' 'endemol' 'bayar' 'board' 'celal' 'member'\n",
      " 'advisory' 'logging' 'excellence' 'administration' 'illinois' 'western'\n",
      " 'lead' 'official' 'science' 'victoria' 'bachelor' 'wellington' 'biology'\n",
      " 'junior' 'mes' 'engineer' 'success' 'set' 'always' 'brand' 'tobacco'\n",
      " 'portfolio' 'international' 'japan' 'jti' 'executive' 'rrp' 'research'\n",
      " 'styczynski' 'assistant' 'lab' 'undergraduate' 'korea' 'native' 'epik'\n",
      " 'teacher' 'program' 'center' 'representative' 'community' 'long' 'beach'\n",
      " 'medical' 'admissions' 'english' 'woodlands' 'gphr' 'chro' 'houston'\n",
      " 'sphr' 'svp' 'officer' 'energy' 'csr' 'engie' 'marketing'\n",
      " 'communications']\n",
      "\n",
      "\n",
      "Dimension 2:\n",
      "['student' 'generalist' 'humber' 'university' 'chapman' 'college'\n",
      " 'seeking' 'hris' 'positions' 'internship' 'advisory' 'board' 'member'\n",
      " 'celal' 'bayar' 'management' 'state' 'westfield' 'loparex' 'schwan'\n",
      " 'scottmadden' 'inc' 'opportunities' 'paint' 'kokomo' 'indiana' 'delphi'\n",
      " 'hardware' 'western' 'official' 'illinois' 'lead' 'bachelor' 'biology'\n",
      " 'science' 'victoria' 'wellington' 'position' 'within' 'patient'\n",
      " 'customer' 'service' 'care' 'employment' 'relocation' 'travel' 'open'\n",
      " 'talent' 'benefits' 'procedures' 'conflict' 'policies' 'compensation'\n",
      " 'national' 'office' 'army' 'guard' 'recruiter' 'retired' '709' 'nortia'\n",
      " 'professionals' 'payroll' '408' 'administrative' '2621' 'undergraduate'\n",
      " 'styczynski' 'assistant' 'lab' 'research' 'set' 'success' 'always'\n",
      " 'executive' 'rrp' 'international' 'brand' 'japan' 'jti' 'portfolio'\n",
      " 'tobacco' 'teacher' 'epik' 'native' 'korea' 'program' 'beach' 'center'\n",
      " 'community' 'representative' 'long' 'medical' 'admissions' 'sphr'\n",
      " 'officer' 'houston' 'communications' 'gphr' 'energy' 'marketing' 'svp'\n",
      " 'chro' 'csr' 'engie' 'woodlands' 'english' 'logging' 'excellence'\n",
      " 'administration' 'engineer' 'mes' 'junior' '2020' 'st' 'entry' 'level'\n",
      " 'graduating' 'louis' 'may' 'groupe' 'beneteau' 'retail' 'travelers'\n",
      " 'analytics' 'intelligence' 'ey' 'analyst' 'liberal' 'arts' 'director'\n",
      " 'work' 'create' 'inclusive' 'passionate' 'engaging' 'helping'\n",
      " 'environment' 'shine' 'endemol' 'information' 'systems' 'data' 'love'\n",
      " 'organization' 'programmer' 'north' 'america' 'major' 'world' 'gis'\n",
      " 'software' 'energetic' 'team' 'focused' 'manager' 'partner'\n",
      " 'environmental' 'heil' 'staffing' 'experienced' 'recruiting' 'leader'\n",
      " 'buckhead' 'intercontinental' 'atlanta' 'luxottica' 'ryan' 'people'\n",
      " 'development' 'coordinator' 'resources' 'human' 'business' '2019' 'magna'\n",
      " 'cum' 'laude' 'bauer' 'graduate' 'aspiring' 'hr' 'senior' 'professional'\n",
      " 'specialist']\n"
     ]
    }
   ],
   "source": [
    "# SVD allows to extract important and informative words\n",
    "# SVD is an algebraic transformation similar to PCA that can be used to find linear combinations of the terms that are informative, \n",
    "# so that we can describe the dataset with fewer combinations than the number of terms we originally had. \n",
    "# These combinations can be considered as dimensions with latent semantic dimensions, that is, dimensions in which it makes sense \n",
    "# to project the dataset precisely because of its semantic content.\n",
    "\n",
    "# The reason why we can reduce the dimensionality of the texts by projecting them to these latent semantic dimensions is that many times\n",
    "# there is redundancy in the set of documents. That is to say that with more or less different words, many documents talk about the same topics. \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd=TruncatedSVD(n_components=2);\n",
    "P=svd.fit_transform(tfidf_encoding)\n",
    "\n",
    "comp1, comp2=svd.components_ # coeficientes (pesos) de los términos en cada una de las dos dimensiones\n",
    "\n",
    "indices=np.argsort(comp1); # los ordenamos de menor a mayor y nos quedamos con los índices de sus posiciones en el array\n",
    "indices=indices[::-1] # invertimos para que queden ordenados de mayor a menor\n",
    "\n",
    "print('Dimension 1:')\n",
    "print(np.array(vectorizer.get_feature_names_out())[indices]) # Evaluamos los términos en estas posiciones\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "indices=np.argsort(comp2);\n",
    "indices=indices[::-1]\n",
    "print('Dimension 2:')\n",
    "print(np.array(vectorizer.get_feature_names_out())[indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('apziva-p1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58a6346ebf7422e603e868fb3854851aa5811b3024e6952b5df4a7c1ed6fa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
