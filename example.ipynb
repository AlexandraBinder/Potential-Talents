{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you? I am Romeo.nHello, Romeo My name is Juliet. Nice to meet you.nNice meet you too. How are you today?nGreat. My baseball team won the competition.nOh Congratulations, JulietnThanks you Romeo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\n",
    "       'Hello, how are you? I am Romeo.n'\n",
    "       'Hello, Romeo My name is Juliet. Nice to meet you.n'\n",
    "       'Nice meet you too. How are you today?n'\n",
    "       'Great. My baseball team won the competition.n'\n",
    "       'Oh Congratulations, Julietn'\n",
    "       'Thanks you Romeo'\n",
    "   )\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you i am romeo', 'hello romeo my ', 'ame is juliet ', 'ice to meet you', '', 'ice meet you too how are you today', 'great my baseball team wo', ' the competitio', '', 'oh co', 'gratulatio', 's juliet', 'tha', 'ks you romeo']\n",
      "\n",
      "['hello', 'how', 'are', 'you', 'i', 'am', 'romeo', 'my', 'ame', 'is', 'juliet', 'ice', 'to', 'meet', 'too', 'today', 'great', 'baseball', 'team', 'wo', 'the', 'competitio', 'oh', 'co', 'gratulatio', 's', 'tha', 'ks']\n"
     ]
    }
   ],
   "source": [
    "sentences = re.sub(\"[.,!?-]\", '', text.lower()).split('n')  # filter '.', ',', '?', '!'\n",
    "print(sentences)\n",
    "print()\n",
    "word_list = list(OrderedDict.fromkeys(\" \".join(sentences).split()))\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1252,  1184,  1164,  1248,  6462,   136,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1790,   112,   189,  1341,  1119,  3520,  1164,  1248,  6462,\n",
      "           117, 21902,  1643,   119,   102],\n",
      "        [  101,  1327,  1164,  5450, 23434,   136,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] But what about second breakfast? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "def token_embedding(sentences):\n",
    "    # INPUT_IDS: are the indices corresponding to each token in the sentence.\n",
    "    # ATTENTION_MASK: indicates whether a token should be attended to or not.\n",
    "    # TOKEN_TYPE_IDS: identifies which sequence a token belongs to when there is more than one sequence.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # print(encoded_input)\n",
    "    return encoded_input\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "token_embedding(batch_sentences)\n",
    "# tokenizer.decode(encoded_input[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pretrained model \n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "   batch = []\n",
    "   positive = negative = 0\n",
    "   while positive != batch_size/2 or negative != batch_size/2:\n",
    "       tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
    "\n",
    "       tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "       input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "       segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "       # MASK LM\n",
    "       n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "       cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                         if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "       shuffle(cand_maked_pos)\n",
    "       masked_tokens, masked_pos = [], []\n",
    "       for pos in cand_maked_pos[:n_pred]:\n",
    "           masked_pos.append(pos)\n",
    "           masked_tokens.append(input_ids[pos])\n",
    "           if random() < 0.8:  # 80%\n",
    "               input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "           elif random() < 0.5:  # 10%\n",
    "               index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "               input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "       # Zero Paddings\n",
    "       n_pad = maxlen - len(input_ids)\n",
    "       input_ids.extend([0] * n_pad)\n",
    "       segment_ids.extend([0] * n_pad)\n",
    "\n",
    "       # Zero Padding (100% - 15%) tokens\n",
    "       if max_pred > n_pred:\n",
    "           n_pad = max_pred - n_pred\n",
    "           masked_tokens.extend([0] * n_pad)\n",
    "           masked_pos.extend([0] * n_pad)\n",
    "\n",
    "       if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "           positive += 1\n",
    "       elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "           negative += 1\n",
    "   return batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('apziva-p1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d58a6346ebf7422e603e868fb3854851aa5811b3024e6952b5df4a7c1ed6fa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
